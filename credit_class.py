# -*- coding: utf-8 -*-
"""credit_Class.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1mmwdcdi8WTShTgRy_H9Ne-74mjrjfzUj

### **Libararies and Data:**
"""

# import libararies
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import lightgbm as lgb
from sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix

from time import time
import warnings
warnings.filterwarnings('ignore')
pd.set_option("max_columns",0)

# mount drive
from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Naya - Keren Ohad Rotem/CSV/df_final_eda.csv')
df=df.drop(columns=['Unnamed: 0'])

df

df.columns

def time_stamp_decoration(func):
    def add_time_stamp(*args, **kwargs):
        t_start = time()
        print(f'Executing {func.__name__}...', end=' ')
        ret = func(*args, **kwargs)
        print("completed! ({:.3f} seconds)".format(time() - t_start))
        return ret
    return add_time_stamp

"""### **Pipeline functions:**

#### **Scaling**
"""

@time_stamp_decoration
def scaler(df_train, df_test, scaler, col, idx_train, idx_test):
  """
    Scaler

    input:
      scaler: a scaler object after call 
      (Ex: scaler:scaler_MM after run outside scaler_MM = MinMaxScaler())
      col: list of the columns dataframe
      idx_train/test: list of the indexes train/test dataframe
    
    output:
      sc_train: data_train scaled by fitted scaler
      sc_test: data_test scaled by fitted scaler
  """

  scaler.fit(df_train)
  sc_train = scaler.transform(df_train)
  sc_train = pd.DataFrame(sc_train, columns = col, index = idx_train)
  sc_test = scaler.transform(df_test)
  sc_test = pd.DataFrame(sc_test, columns = col, index = idx_test)
  
  return (sc_train, sc_test)

"""#### **Cross Validation**"""

@time_stamp_decoration
def CV_model(x, y, k_fold, model, **model_params):
  """
    Cross Validation

    input:
      model: a model object after call
      (Ex: model:model_Log after run outside model_Log = LogisticRegression())
      model_params: a dictionary of model params

    output:
      model_proba: an array with dim(n_samples, n_classes) of probabilities
  """

  kf = StratifiedKFold(n_splits=k_fold, shuffle=True)
  scores = cross_val_score(model ,x, y, fit_params=model_params, cv=kf, scoring='roc_auc')
  proba = cross_val_predict(model ,x, y, fit_params=model_params, cv=kf, method='predict_proba')[:, 1]
  print("Scores : " + (k_fold * "{:.3f} ").format(*scores))
  # model.fit(x, y, fit_params=model_params)
  # proba = model.predict_proba(x)[:, 1]

  return (proba)

"""#### **Grid Search**"""

@time_stamp_decoration
def GS_model(x, y, k_fold, model, **grid_params):
  """
    Grid Search
    
    input:
      model: a model object after call
      (Ex: model:model_Log after run outside model_Log = LogisticRegression())
      grid_params: a dictionary of model params

    output:
      model_proba: an array with dim(n_samples, n_classes) of probabilities
      gs_model: fitted model
  """

  kf= StratifiedKFold(n_splits=k_fold, shuffle=True)
  gs_model = GridSearchCV(model, param_grid=grid_params, return_train_score=True,
                          cv=kf, scoring='roc_auc')
  gs_model.fit(x, y)
  print("Best parameters:", gs_model.best_params_)
  print("Train_scores:", gs_model.cv_results_['mean_train_score'])
  print("Test_scores:", gs_model.cv_results_['mean_test_score'])
  print("Best_score:", gs_model.best_score_)
  proba = gs_model.predict_proba(x)[:, 1]

  return (gs_model, proba)

"""#### **Report**"""

@time_stamp_decoration
def report(x, y, proba, fitted_model):
  """
  report model
  """
  yp = fitted_model.predict(x)
  auc = roc_auc_score(y_true=y, y_score=proba)
  cm = pd.DataFrame(confusion_matrix(y_true=y, 
                                      y_pred=yp), 
                    index=fitted_model.classes_, 
                    columns=fitted_model.classes_)
  rep = classification_report(y_true=y, 
                              y_pred=yp)
  return 'roc_auc_score: {:.3f}\n\n{}\n\n{}'.format(auc, cm, rep)

"""#### **Feature Importance**"""

@time_stamp_decoration
def featureImp(x, model, fitted_model):
  """
  plot the most 15 important features of the model
  """
  sort = fitted_model.best_estimator_.feature_importances_.argsort()[:15]
  plt.barh(x.columns[sort], fitted_model.best_estimator_.feature_importances_[sort])
  plt.title('Feature Importance '+model.__class__.__name__, fontsize = 20)
  return()

"""#### **ROC Curve**"""

@time_stamp_decoration
def ROC_curves(y, **models_proba):
  """
  plot multiple ROC curves for multiple models
  """

  plt.style.use('seaborn')
  colors = {0:'green', 1:'red', 2:'orange', 3:'purple', 4:'brown', 5:'pink', 6:'cyan'}
  
  random_proba = [0 for i in range(len(y))]
  p_fpr, p_tpr, _ = roc_curve(y_true=y, y_score=random_proba, pos_label=1)
  plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')

  for i, (k, v) in enumerate(models_proba.items()):
    fpr, tpr, _ = roc_curve(y_true=y, y_score=v, pos_label=1)
    plt.plot(fpr, tpr, linestyle='--',color=colors[i], label=k)

  # title
  plt.title('ROC curve', fontsize = 20)
  # x label
  plt.xlabel('False Positive Rate', fontsize = 16)
  # y label
  plt.ylabel('True Positive rate', fontsize = 16)

  plt.xlim([0, 1])
  plt.ylim([0, 1])
  plt.legend(loc='best', fontsize = 14)
  plt.show();

  return()
# https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/  - ploting many AUC to see them agains wech other

"""### **Train models:**

#### **Preprocessing**
"""

# Split the data to train and test sets:
X=df.drop('TARGET', axis=1)
y=df.TARGET
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25, 
                                                    random_state=123456, shuffle=True, stratify=y)

# Scaling:
scaler_MM = MinMaxScaler()
sc_train, sc_test = scaler(X_train, X_test, scaler_MM, X_train.columns, X_train.index, X_test.index)

# Set k_fold parameter:
k = 5

"""#### **Logistic Regression**"""

model_Log = LogisticRegression()
grid_param_Log = {'penalty': ['l2'],
                  'C': [1.0],
                  'class_weight': ['balanced'],
                  'solver': ['saga']
                  }

fitted_model_Log, proba_Log = GS_model(sc_train, y_train, k, model_Log, **grid_param_Log)
print(report(sc_train, y_train, proba_Log, fitted_model_Log))

"""#### **Random Forest**"""

model_RF = RandomForestClassifier()
grid_param_RF = {'n_estimators': [50],
                 'max_depth': [10],
                 'class_weight': ['balanced'], 
                 'min_weight_fraction_leaf': [0.01]
                 }
fitted_model_RF, proba_RF = GS_model(sc_train, y_train, k, model_RF, **grid_param_RF)
print(report(sc_train, y_train, proba_RF, fitted_model_RF))
featureImp(sc_train, model_RF, fitted_model_RF)

"""#### **XGBoost**"""

model_XGB = xgb.XGBClassifier()
grid_param_XGB = {'colsample_bytree': [0.3],
                  'learning_rate': [0.1],
                  'max_depth': [5],
                  'class_weight': [{1:20}], 
                  'alpha': [10]
                  }
fitted_model_XGB, proba_XGB = GS_model(sc_train, y_train, k, model_XGB, **grid_param_XGB)
print(report(sc_train, y_train, proba_XGB, fitted_model_XGB))
featureImp(sc_train, model_XGB ,fitted_model_XGB)

"""#### **LightGBM**"""

model_LGBM = lgb.LGBMClassifier()
grid_param_LGBM = {'objective': ['binary'],
                   'class_weight': [{1:20}],
                   'boosting': ['gbdt'],
                   'metric': ['auc'],
                   'num_leaves': [60],
                   'learning_rate': [0.05],
                   'feature_fraction': [0.5],
                   'bagging_fraction': [1],
                   'bagging_freq': [1],
                   'verbose': [-5]
                   }
fitted_model_LGBM, proba_LGBM = GS_model(sc_train, y_train, k, model_LGBM, **grid_param_LGBM)
print(report(sc_train, y_train, proba_LGBM, fitted_model_LGBM))
featureImp(sc_train, model_LGBM ,fitted_model_LGBM)

"""### **ROC Curves Figure of all the models:**

#### **Train**
"""

dic_models = {'Logistic Regression': proba_Log,
              'RF': proba_RF,
              'XGBoost': proba_XGB,
              'LightGBM': proba_LGBM
              }
ROC_curves(y_train, **dic_models)

"""#### **Test**"""

proba_test_Log = fitted_model_Log.predict_proba(sc_test)[:, 1]
proba_test_RF = fitted_model_RF.predict_proba(sc_test)[:, 1]
proba_test_XGB = fitted_model_XGB.predict_proba(sc_test)[:, 1]
proba_test_LGBM = fitted_model_LGBM.predict_proba(sc_test)[:, 1]

# Logistic Regression
print(report(sc_test, y_test, proba_test_Log, fitted_model_Log))

# Random Forest
print(report(sc_test, y_test, proba_test_RF, fitted_model_RF))

# XGBoost
print(report(sc_test, y_test, proba_test_XGB, fitted_model_XGB))

# LightGBM
print(report(sc_test, y_test, proba_test_LGBM, fitted_model_LGBM))

dic_test_models = {'Logistic Regression': proba_test_Log,
                   'RF': proba_test_RF,
                   'XGBoost': proba_test_XGB,
                   'LightGBM': proba_test_LGBM
                   }
ROC_curves(y_test, **dic_test_models)